{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUR46cM4nSd5URFpWbvePs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oshsanjuan/RAG-Job-Qualification-AI/blob/main/Automating_Job_Qualification_Answers_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Model for Introducing Yourself to Potential Employers\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This notebook is an end-to-end walkthrough of a RAG pipeline, from the indexing, to the query until demo deployment on Gradio. It is a simple RAG pipeline set to answer questions on the user based on their data. In my case I'm making use of my resume, cover letter as well as the job description.\n",
        "\n",
        "\n",
        "## Preparing the notebook\n",
        "\n",
        "This project was mainly done on Google Colaboratory and as such have some code specific to it. As seen below the notebook must have the user's drive mounted or connected to it to be able to access their storage.\n",
        "\n",
        "Google Drive was utilized to store necessary documents to be processed (i.e. Resume, Cover Letter)"
      ],
      "metadata": {
        "id": "Uw8uk8k4K5Iy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXxqkpHlJy3V",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Mounting the drive to access data storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, to help in debugging we import logging. This is recommend by Haystack (the open source framework we will be using) for the implementation of their modules."
      ],
      "metadata": {
        "id": "F8eKGJDnLp6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For better debugging using Haystack\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\",\n",
        "                    level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "j5NSbulCKEV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages and Dependencies\n",
        "\n",
        "Let's install the necessary dependencies. We'll be making use of the following packages.\n",
        "\n",
        "One of the first decisions to make when creating a RAG pipeline is what framework to use, as mentione dearlier we will be using haystack."
      ],
      "metadata": {
        "id": "mEXsjdVPMJ6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install haystack-ai\n",
        "pip install \"sentence-transformers>=3.0.0\"\n",
        "pip install google-ai-haystack\n",
        "pip install markdown-it-py mdit_plain pypdf\n",
        "pip install gdown\n",
        "pip install trafilatura\n",
        "pip install chroma-haystack\n",
        "pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0ZTGiFFtKfvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we start importing the necessary modules. The first import from google is important as it is how we will be accessing Colab's secrets.\n",
        "\n",
        "As we will see later there will be instances where API Keys will be necessary, but we don't want them to be visible in the code. There are other solutions to this, but given that Colab has the secrets function we will utilize that."
      ],
      "metadata": {
        "id": "XHf--CumMU5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import this if you're using Colab so you can retrieve stored Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "\n",
        "from haystack import Pipeline\n",
        "from haystack.components.converters import HTMLToDocument\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument\n",
        "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
        "from haystack.components.routers import FileTypeRouter\n",
        "from haystack.components.joiners import DocumentJoiner\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.fetchers import LinkContentFetcher\n",
        "from haystack_integrations.document_stores.chroma import ChromaDocumentStore"
      ],
      "metadata": {
        "id": "PDTGo9ofKwm0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Indexing Pipeline\n",
        "\n",
        "We start with an Indexing Pipeline, where we will take our data, convert them into a format our AI will understand, and place them into a storage.\n",
        "\n",
        "Haystack offers several modules, for our purposes will be making use of ChromaDB as can be seen in this import:\n",
        "\n",
        "`from haystack_integrations.document_stores.chroma import ChromaDocumentStore`\n",
        "\n",
        "## Setting up your data storage\n",
        "\n",
        "Since we're using Colab we've got our PDFs on Google Drive, and so we set the folder we've got them on as the path.\n",
        "\n",
        "With this our ChromaDB Document Store is set to place our pre-processed documents in `chromaDB_path`\n"
      ],
      "metadata": {
        "id": "ZkKA9D1QNME3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd drive/MyDrive/'Colab Notebooks'\n",
        "chromaDB_path = \"/content/drive/MyDrive/Colab Notebooks/OBZ Exam/Data Storage/chromaDB\"\n",
        "document_store = ChromaDocumentStore(persist_path=chromaDB_path)"
      ],
      "metadata": {
        "id": "PFGsYVqJNK13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File Handling\n",
        "\n",
        "While the files in the storage are primarily pdfs, we want to ensure our model is able to handle different types of input. We'll be using `FileTypeRouter` from Haystack which routes files paths or byte streams based on their type to the appropriate output for processing."
      ],
      "metadata": {
        "id": "ak51z9ckQdkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_type_router = FileTypeRouter(mime_types=[\"text/plain\",\n",
        "                                              \"application/pdf\",\n",
        "                                              \"text/markdown\"])"
      ],
      "metadata": {
        "id": "98Gz52zAO9Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The files will be routed to one of these converters:\n",
        "\n",
        "Converters extract the data from the file and convert them to a document format that haystack makes use of. Then the document_joiner takes their outputs and unifies them so that only a single output goes towards the cleaner."
      ],
      "metadata": {
        "id": "o0A0rVDZRyYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file_converter = TextFileToDocument()\n",
        "markdown_converter = MarkdownToDocument()\n",
        "pdf_converter = PyPDFToDocument()\n",
        "html_to_document= HTMLToDocument()\n",
        "document_joiner = DocumentJoiner()"
      ],
      "metadata": {
        "id": "HnpqWVmyRdcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For cleaning these documents Haystack provides a DocumentCleaner function. It also provides for a document splitter which divides the documents into lists of shorter text documents so that our LLMs can process them faster.\n",
        "\n",
        "Our documents are not that lengthy or we split them into small chunks and with some overlap just so that we maintains some semantic context between words."
      ],
      "metadata": {
        "id": "ZdmYN2VwSRDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_cleaner = DocumentCleaner()\n",
        "document_splitter = DocumentSplitter(\n",
        "    split_by=\"word\",\n",
        "    split_length=150,\n",
        "    split_overlap=50\n",
        "    )"
      ],
      "metadata": {
        "id": "dPbVa9HFSLcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need an embedder because our LLMs don't actually read the words but their vector representation, I've kept it simple and used the default model Haystack tends to use in their documentation.\n",
        "\n",
        "Finally, we need a document_writer or a function that will place these documents into our document store."
      ],
      "metadata": {
        "id": "UoQT_ig9TTmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "document_writer = DocumentWriter(document_store)"
      ],
      "metadata": {
        "id": "FoCtxZS1TLgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together\n",
        "\n",
        "We put the pipeline together, by instantiating it as components:"
      ],
      "metadata": {
        "id": "rLHjVrg7T6Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_pipeline = Pipeline()\n",
        "preprocessing_pipeline.add_component(instance=file_type_router,\n",
        "                                     name=\"file_type_router\")\n",
        "preprocessing_pipeline.add_component(instance=text_file_converter,\n",
        "                                     name=\"text_file_converter\")\n",
        "preprocessing_pipeline.add_component(instance=markdown_converter,\n",
        "                                     name=\"markdown_converter\")\n",
        "preprocessing_pipeline.add_component(instance=pdf_converter,\n",
        "                                     name=\"pypdf_converter\")\n",
        "preprocessing_pipeline.add_component(instance=html_to_document,\n",
        "                                     name=\"html_to_document\")\n",
        "preprocessing_pipeline.add_component(instance=document_joiner,\n",
        "                                     name=\"document_joiner\")\n",
        "preprocessing_pipeline.add_component(instance=document_cleaner,\n",
        "                                     name=\"document_cleaner\")\n",
        "preprocessing_pipeline.add_component(instance=document_splitter,\n",
        "                                     name=\"document_splitter\")\n",
        "preprocessing_pipeline.add_component(instance=document_embedder,\n",
        "                                     name=\"document_embedder\")\n",
        "preprocessing_pipeline.add_component(instance=document_writer,\n",
        "                                     name=\"document_writer\")"
      ],
      "metadata": {
        "id": "U2otgqusUEWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then connect them, which is done by taking the output of one component and placing it into the input of another.\n",
        "\n",
        "It should be important to note here that Haystack only allows one component to be connected to another component, meaning I can't connect `document_joiner` to `document_cleaner` and then `document_splitter` at the same time.\n",
        "\n",
        "There are exceptions for `document_joiner` where it takes several outputs from our different converters."
      ],
      "metadata": {
        "id": "VAmQssLXUS-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect the components, output to input\n",
        "preprocessing_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
        "preprocessing_pipeline.connect(\"file_type_router.application/pdf\", \"pypdf_converter.sources\")\n",
        "preprocessing_pipeline.connect(\"file_type_router.text/markdown\", \"markdown_converter.sources\")\n",
        "preprocessing_pipeline.connect(\"file_type_router.text/plain\", \"html_to_document.sources\")\n",
        "# preprocessing_pipeline.connect(\"link_content_fetcher.streams\", \"html_to_document_link\")\n",
        "preprocessing_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
        "preprocessing_pipeline.connect(\"html_to_document\", \"document_joiner\")\n",
        "preprocessing_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
        "preprocessing_pipeline.connect(\"markdown_converter\", \"document_joiner\")\n",
        "preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
        "preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
        "preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
        "preprocessing_pipeline.connect(\"document_embedder\", \"document_writer\")"
      ],
      "metadata": {
        "id": "v4jwPgnKUSfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run the pipeline and our very first component requires an input, in this case it is the folder on my google drive where the pdfs are stored.\n",
        "\n",
        "We use the `glob()` method to get any  and all files and directories inside the folder.\n",
        "\n",
        "We run the pipeline and find ourselves with 11 documents written and placed in our document store."
      ],
      "metadata": {
        "id": "ZVQ51H2RVI55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/OBZ Exam/Data Storage\"\n",
        "preprocessing_pipeline.run(\n",
        "    {\"file_type_router\": {\"sources\": list(Path(data_path).glob(\"**/*\"))}}\n",
        "    )"
      ],
      "metadata": {
        "id": "RPpwb6FxVu_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Query Pipeline\n",
        "\n",
        "Now that our Indexing Pipeline is setup, we move onto creating our Query Pipeline, where we setup the LLM that will interacting with our data storage, and answering questions based on the context we provide.\n",
        "\n",
        "## Embedder\n",
        "\n",
        "We set up an embedder to convert our documents into vector representations, which our LLM (Large Language Model) can process and understand. The embedder we're using, the SentenceTransformersTextEmbedder, requires a Hugging Face API key since the model is hosted on Hugging Face's platform. I securely stored my API key in the environment and accessed it as an environment variable, ensuring smooth access to the embedding model.\n",
        "\n",
        "you can run the `warm_up()` to see if the embedder is working properly."
      ],
      "metadata": {
        "id": "-vIATe_jWilv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "embedder = SentenceTransformersTextEmbedder(\n",
        "    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "# embedder.warm_up()"
      ],
      "metadata": {
        "id": "TyfWqlaAV94Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever\n",
        "\n",
        "Our choice of retriever is limited as we have to match it to our choice of data storage, in this case we make use of the `ChromaEmbeddingRetriever`."
      ],
      "metadata": {
        "id": "tKbqgCiSX3oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack_integrations.components.retrievers.chroma import ChromaEmbeddingRetriever\n",
        "retriever = ChromaEmbeddingRetriever(document_store=document_store)"
      ],
      "metadata": {
        "id": "yzvw7DI5X3Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Template\n",
        "\n",
        "We need to build prompts for interacting with LLMs  and so we import the `PromptBuilder` from haystack.\n",
        "\n",
        "It makes use of the Jinja Template as its structure, it is a set of placeholders that are filled when the the template is used. The full documentation for Jinja can be found [here](https://jinja.palletsprojects.com/en/3.0.x/templates/)"
      ],
      "metadata": {
        "id": "bS2o1g34YLTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.builders import PromptBuilder\n",
        "\n",
        "template = \"\"\"\n",
        "You are designed to answer questions about a potential candidate for the position of\n",
        "Junior AI Engineer at OneByZero. Use the information provided to you to answer these questions\n",
        "to the best of your ability to speak about the candidate.\n",
        "\n",
        "Context:\n",
        "{% for document in documents %}\n",
        "    {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{ question }}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt_builder = PromptBuilder(template=template)"
      ],
      "metadata": {
        "id": "8OoNzV32aMtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator\n",
        "\n",
        "Haystack makes use of several llms, seeing as we're working primarily on Colab, I'll be making use of Gemini (that and it's free, sorry OpenAI). Specifically I made use of `gemini-1.5-flash` model.\n",
        "\n",
        "We import the `GoogleAIGeminiGenerator from Haystack, setup the necessary API Key and place the model in its parameters."
      ],
      "metadata": {
        "id": "yyzbA57oaWOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remember to get the Gemini dependency\n",
        "from haystack_integrations.components.generators.google_ai import GoogleAIGeminiGenerator\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "generator = GoogleAIGeminiGenerator(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "Rzh8BqLkbNuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting the Query Pipeline together\n",
        "\n",
        "With all the parts ready we instantiate and connec them like our Indexing Pipeline."
      ],
      "metadata": {
        "id": "S6gZRPdrbXxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the RAG pipeline\n",
        "query_pipeline = Pipeline()\n",
        "query_pipeline.add_component(\"embedder\", embedder)\n",
        "query_pipeline.add_component(\"retriever\", retriever)\n",
        "query_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
        "query_pipeline.add_component(\"llm\", generator)\n",
        "\n",
        "#connect them\n",
        "query_pipeline.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
        "query_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "query_pipeline.connect(\"prompt_builder.prompt\", \"llm\")"
      ],
      "metadata": {
        "id": "klKZyIepaSnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The whole pipeline is complete at this point, and we can test it out. Let's start by asking something simple like who is the job candidate."
      ],
      "metadata": {
        "id": "5foP7N3JcsnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = (\n",
        "    \"Who is the candidate for the position?\"\n",
        "    )\n",
        "response = query_pipeline.run(\n",
        "    {\n",
        "        \"embedder\": {\"text\": question},\n",
        "        \"prompt_builder\": {\"question\": question}\n",
        "    }\n",
        ")\n",
        "response[\"llm\"][\"replies\"][0]"
      ],
      "metadata": {
        "id": "2YMkXc0Kcpl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a bit hard to read, but it got the job done. We're ready to place it on our demo application.\n",
        "\n",
        "For the purposes of this demonstration we'll be making use of Gradio as I found it to be most compatible with a Haystack pipeline.\n",
        "\n",
        "However, before we place our pipeline onto Gradio we just need to add one last feature.\n",
        "\n",
        "## Chat history\n",
        "\n",
        "If you go back to our initial `query_pipeline` and ask it to tell you what the previous question was, our model will not be able to answer because it is not part of its context window.\n",
        "\n",
        "To account for this we add a `chat_history` to store the previous messages and adjust our pipeline a little to account for this, we start referring to it as a prompt since, we account not just for the question but also the chat history."
      ],
      "metadata": {
        "id": "f4bMWzSGdFXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store chat history\n",
        "chat_history = []\n",
        "\n",
        "def generate_answer(message, history):\n",
        "    # Add the current message to the chat history\n",
        "    chat_history.extend([(\"user\", message)])\n",
        "\n",
        "    # Include history in the prompt (adjust as needed)\n",
        "    prompt = f\"\"\"\n",
        "    ## Chat History:\n",
        "    {format_history(history)}\n",
        "\n",
        "    ## User's Question:\n",
        "    {message}\n",
        "    \"\"\"\n",
        "\n",
        "    result = query_pipeline.run(\n",
        "        {\n",
        "            \"embedder\": {\"text\": str(prompt)},  # Embed the prompt with history\n",
        "            \"prompt_builder\": {\"question\": prompt}\n",
        "        }\n",
        "    )\n",
        "\n",
        "    answer = result[\"llm\"][\"replies\"][0]\n",
        "\n",
        "    # Add the answer to the chat history\n",
        "    chat_history.extend([(\"assistant\", answer)])\n",
        "\n",
        "    # Return only the answer, Gradio handles history\n",
        "    return answer  # or answer, history\n",
        "\n",
        "def format_history(history):\n",
        "    \"\"\"Formats the chat history for the prompt.\"\"\"\n",
        "    formatted_history = \"\"\n",
        "    for role, content in history:\n",
        "        formatted_history += f\"{role.capitalize()}: {content}\\n\"\n",
        "    return formatted_history"
      ],
      "metadata": {
        "id": "FiuPb9PncAhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment Demo with Gradio\n",
        "\n",
        "The model is ready to be deployed and thankfully Gradio provides a `ChatInterface()` to go with our very own RAG AI application.\n",
        "\n",
        "Let's import it first, and then set it up.\n"
      ],
      "metadata": {
        "id": "ciTfutsQgUTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make sure to import it first\n",
        "import gradio as gr\n",
        "\n",
        "#Setting up the Chatbot Interface\n",
        "chatbot_with_gemini = gr.ChatInterface(\n",
        "    generate_answer,\n",
        "    chatbot=gr.Chatbot(height=300),\n",
        "    textbox=gr.Textbox(placeholder='Ask me a question about this candidate', container=False, scale=7),\n",
        "    title=\"RAG AI Chatbot powered by Gemini\",\n",
        "    description=\"Ask me about Osh and his qualifications\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\n",
        "        \"What is his work experience?\",\n",
        "        \"What is his education?\",\n",
        "        \"What are his skills?\",\n",
        "    ],\n",
        "    cache_examples=False,\n",
        "    submit_btn=\"Ask\",\n",
        "    multimodal=True\n",
        ")"
      ],
      "metadata": {
        "id": "x_lokWVcckTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lot of the parameters are mostly for aesthetic reasons, what's important for this is the first one, `generate_answer` which is our function that calls on the LLM to answer the queries.\n",
        "\n",
        "Now let's launch it and try it out.\n",
        "\n",
        "We don't have to set `debut=True` but incase anything goes wrong, at least we'll have an idea of why it happened."
      ],
      "metadata": {
        "id": "zgXZv5kUhPuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot_with_gemini.launch(debug=True)"
      ],
      "metadata": {
        "id": "rka3jsD3hPXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there we have it a full RAG pipeline.\n",
        "\n",
        "# Methods for Evaluation\n",
        "\n",
        "To evaluate this pipeline, there are several metrics, I'll be taking mine from those suggested by Haystack [here](https://haystack.deepset.ai/tutorials/35_evaluating_rag_pipelines) ,as they do provide functions to create an evaluation pipeline (which I may add in the near future).\n",
        "\n",
        "\n",
        "\n",
        "*   **Document Mean Reciprocal** : This evaluates the documents the model pulls from the storage and checks how they were ranked.\n",
        "*   **Semantic Answer Similarity**: This checks if the answer provided shares similar semantics to the document it pulled.\n",
        "*   **Faithfulness**: Makes use of an LLM to check if the answer can be inferred from the context (does not need ground truth labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sOATNm0JhetQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_lHet8T6kF0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4o0UU1BFhB7A"
      }
    }
  ]
}